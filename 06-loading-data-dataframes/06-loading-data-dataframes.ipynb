{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "50a40f10-f4b6-4dd3-b7aa-c63ed3ca3244"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Data Science - Lecture 6 - Loading Data, Dataframes\n",
    "*COMP 5360/ MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "In this lecture, we will learn how to read in and write files and then finally cover pandas dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Series Recap\n",
    "\n",
    "In the previous lab we've introduced pandas series, a one-dimensional data structure. You will see that dataframes behave very similar to series, so we will only quickly recap the most important aspects.\n",
    "\n",
    "Let's start by importing pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series have an index, and associated data. We can use the index to access the data. We've also learned that we can directly access indices and values through their implicit location. If we add the implicit location, a series looks like this: \n",
    "\n",
    "\n",
    "| Location (Implicit)| Index | Value | \n",
    "| - | - | - |\n",
    "|0| Stones     |    1962\n",
    "|1| Beatles    |    1960\n",
    "|2| Zeppelin     |  1968\n",
    "|3| Pink Floyd |    1965\n",
    "|4| Pink Floyd |    2012\n",
    "\n",
    "Here is the example in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_founded = pd.Series([1962, 1960, 1968, 1965, 2012],\n",
    "                         name=\"founded\",\n",
    "                         index=[\"Stones\", \"Beatles\", \"Zeppelin\", \"Pink Floyd\", \"Pink Floyd\"])\n",
    "bands_founded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we access data with multiple indices, we don't get a simple data type, as in the above cases, but instead get another series back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_founded[\"Pink Floyd\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and slicing\n",
    "\n",
    "Indexing and slicing works largely like in normal python, but instead of just directly using the bracket notations, it is recommended to use `iloc` for indexing by position and `loc` for indexing by index. \n",
    "\n",
    "Read the documentation on [loc](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html) and [iloc](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iloc.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing by position\n",
    "bands_founded.iloc[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing by index - note that the last element is included\n",
    "bands_founded.loc[\"Zeppelin\" : \"Pink Floyd\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series can be indexed with arrays, which isn't possible in vanilla python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_founded.loc[[\"Beatles\", \"Pink Floyd\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking Series\n",
    "\n",
    "With pandas we can create boolean arrays that we can use to mask and filter a dataset. In the following expression, we'll create a new array that has \"True\" for every band formed after 1964 using a technique called **broadcasting**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = bands_founded > 1964\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a boolean mask to filter a series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the mask to the original array\n",
    "bands_founded[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The short form here would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_founded[bands_founded > 1965]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring a Series\n",
    "\n",
    "There are various ways to explore the data in a series. We can get an overview of the statistical properties, or calculate individual properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = pd.Series([1962, 1960, 1968, 1965, 2012, 2016])\n",
    "numbers.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_numbers = numbers.sort_values(ascending=False)\n",
    "sorted_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_numbers.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`drop` is a optional parameter with default value of false. If not passed as `True` it adds a new column with resetted index. Old index is added as a column. **This turns the series into a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_numbers.reset_index() # equivalent to sorted_numbers.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping - Applying a Function\n",
    "\n",
    "Mapping applies a function to every element in a series, which is a very powerful approach: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Convert an integer year into a date, assuming Jan 1 as day and month.\n",
    "def to_date(year):\n",
    "    return datetime.date(int(year), 1, 1)\n",
    "    \n",
    "sorted_numbers.map(to_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reading Data\n",
    "\n",
    "Up to now, we've mainly used data that we've specified directly in code. This is, of course, not particularly scalable. We want to load data from files and eventually also connect to databases and APIs. \n",
    "\n",
    "Data is often stored in structured file formats, such as CSV, JSON, or XML. We'll encounter all of these file formats in this class.\n",
    "\n",
    "JSON\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"fruit\": \"Apple\",\n",
    "    \"size\": \"Large\",\n",
    "    \"color\": \"Red\"\n",
    "}\n",
    "```\n",
    "\n",
    "XML\n",
    "```xml\n",
    "<note>\n",
    "<to>Students</to>\n",
    "<from>Prof</from>\n",
    "<heading>Reminder</heading>\n",
    "<body>HW2 due this Friday at 11:59pm!</body>\n",
    "</note>\n",
    "```\n",
    "\n",
    "The simplest (and least structured) is a CSV - comma separated values - file. CSV isn't a formal file format, rather it's a table represented as a text file where the cells are separated by a delimiter. Commonly, the first row represents the header. A delimiter can be a tab character, a semicolon, a colon, etc. \n",
    "\n",
    "Many CSV files also have a special convention for dealing with text that could include the delimiter. The following text would be very hard to parse otherwise:\n",
    "```\n",
    "Artist, Album, Genre\n",
    "Michael Jackson, Bad, Pop, funk, rock\n",
    "``` \n",
    "\n",
    "Here, the album is of multiple genres which are separated by a comma. The comma, however, is also used to delimit the individual columns. To work around that, double-quotes are commonly used to indicate that all the elements contained within the quotes are not meant to be delimiters:\n",
    "\n",
    "```\n",
    "Artist, Album, Genre\n",
    "Michael Jackson, Bad, \"Pop, funk, rock\"\n",
    "``` \n",
    "\n",
    "Now, it is clear that `Pop, funk, rock` should belong in a single cell. \n",
    "\n",
    "We've prepared a dataset based on Wikipedia's [list of best-selling albums](https://en.wikipedia.org/wiki/List_of_best-selling_albums) in the file [hit_albums.csv](./hit_albums.csv). \n",
    "\n",
    "Here is what the first couple of lines look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Artist,Album,Released,Genre,\"Certified sales (millions)\",Claimed sales (millions)\n",
    "Michael Jackson,Thriller,1982,\"Pop, rock, R&B\",45.4,65\n",
    "AC/DC,Back in Black,1980,Hard rock,25.9,50\n",
    "Pink Floyd,The Dark Side of the Moon,1973,Progressive rock,22.7,45\n",
    "Whitney Houston / Various artists,The Bodyguard,1992,\"Soundtrack/R&B, soul, pop\",27.4,44\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various ways of reading a CSV file. We'll first cover the basic read (and write) operations of Python, but will quickly move on to specific parsers for CSV files in Python and in pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic File Operations\n",
    "\n",
    "To read a file we first have to open it by specifying the file path, and specifying whether we want to read (r), write (w), both (r+), or append (a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albums_file = open('hit_albums.csv', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read a whole file at once. Notice that lines are terminated with a special character, a linefeed or newline character specified as `\\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = albums_file.read()\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print this instead, `\\n` is translated into a newline: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading a file, we have to manually close it again to release the OS resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albums_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can read each line separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albums_file = open('hit_albums.csv', 'r')\n",
    "line1 = albums_file.readline();\n",
    "print(line1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now [`split()`](https://docs.python.org/3/library/stdtypes.html#str.split) the string based on the comma, to create a simple CSV parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can loop over the file and read the data into an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for line in albums_file:\n",
    "    data.append(line.split(\",\"))\n",
    "    \n",
    "# let's not forget to close the file:\n",
    "albums_file.close()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now read individual cells or rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this didn't take proper care of our double-quote escape of \"Pop, rock, R&B\". Also, numbers are still treated as strings and the newline character is also appended to the last cell. \n",
    "\n",
    "We could certainly improve our parser to handle these issues, but fortunately, there are existing methods to parse CSV files that make this easier.\n",
    "\n",
    "### Writing\n",
    "\n",
    "We can write by opening a file using the `w` flag. Here we also use the [`with`](https://docs.python.org/3/reference/compound_stmts.html#the-with-statement) keyword, which also takes care of closing the file for us, even if things go wrong (see [this blog post](https://jeffknupp.com/blog/2016/03/07/python-with-context-managers/) for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_file.txt', 'w') as new_file:\n",
    "    new_file.write(\"Hello World\\nAre you still spinning?\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check out this file by opening [my_file.txt](my_file.txt). Notice that the file is only guaranteed to be written if you actually close it (which, here, is take care of by the context manager invoked by the with statement). \n",
    "\n",
    "You can find more examples on basic file operations in the [Python Documentation](https://docs.python.org/3/tutorial/inputoutput.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Reading and Writing Data\n",
    "\n",
    "The file [grades.csv](grades.csv) is a file with student names and letter grades:\n",
    "\n",
    "```\n",
    "Alice; A\n",
    "Bob; B\n",
    "Robert; A\n",
    "Richard; C\n",
    "```\n",
    "\n",
    "Read the file into an array. Add a GPA to the student's row (A=4,B=3,C=2,D=1). \n",
    "\n",
    "Hint: the function [strip()](https://docs.python.org/3/library/stdtypes.html#str.strip) removes trailing whitespace from a string.\n",
    "\n",
    "Write that file into a new file `grades_gpa.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpas = {\"A\":4, \"B\":3, \"C\":2, \"D\":1}\n",
    "\n",
    "data = []\n",
    "# continue here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a CSV file with the CSV Library\n",
    "\n",
    "We can use the CSV library to help with reading the data. It takes a `delimiter` and a `quotechar`, the latter is useful for our double quotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the csv library\n",
    "import csv\n",
    "\n",
    "# initialize the top-level array\n",
    "data_values = []\n",
    "\n",
    "# open the file and append rows as arrays to the data_values\n",
    "with open('hit_albums.csv') as csvfile:\n",
    "    # note that we can interchangably use ' and \" in general\n",
    "    # for the quotechar, however we use ' so that we can use \" without escaping\n",
    "    filereader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    # the row here is an array\n",
    "    for row in filereader:\n",
    "        print(\"Row: \" + str(row))\n",
    "        data_values.append(row)\n",
    "\n",
    "# Store the header in a separate array\n",
    "header = data_values.pop(0)\n",
    "   \n",
    "print()    \n",
    "print(header)\n",
    "print()\n",
    "print(data_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do computation on the numerical dimensions of this table, we need to also convert the strings to numbers. Here, the last column, `Claimed sales (millions)` doesn't have values for each row. In that case, the conversion throws a `ValueError` exception. [Exceptions](https://docs.python.org/3/reference/compound_stmts.html#try) are error states that can be raised and caught:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in data_values: \n",
    "    row[2] = int(row[2])\n",
    "    row[4] = float(row[4])\n",
    "    # need to try and catch the exception because the column contains NaN values\n",
    "    try:\n",
    "        row[5] = float(row[5])\n",
    "    except ValueError: \n",
    "        row[5] = None\n",
    "    \n",
    "data_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here we have matrix that we could work with. In reality, we probably would want to structure the data a little differently - instead of treating each row as an array, we'd want to treat each dimension (column) as an array, as this makes the column homogeneous and it makes it easy to calculate means, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CSV with Pandas\n",
    "\n",
    "Now, let's take a look at what it takes to read this file using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hit_albums = pd.read_csv(\"hit_albums.csv\")\n",
    "hit_albums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was different! \n",
    "\n",
    "Pandas provides the insanely powerful ['read_csv()'](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) method - also see [this](http://pandas.pydata.org/pandas-docs/stable/io.html) for more info on all I/O operations in pandas, including writing CSV files. \n",
    "\n",
    "You can pass a lot of arguments to the method, such as delimiter, quote-chars, etc., but for our case the default parameters just worked. \n",
    "\n",
    "We've also just created our first data frame! Let's look at data frames in detail next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Frames\n",
    "\n",
    "A data frame is a column-oriented data structure where each column is a pandas series.\n",
    "\n",
    "Here is [a printable cheat sheet for pandas](https://drive.google.com/drive/u/0/folders/0ByIrJAE4KMTtaGhRcXkxNHhmY2M).\n",
    "\n",
    "We've already loaded a data frame from file, but for completeness sake, let's create one in code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandInfo = pd.DataFrame({\n",
    "        \"Name\":[\"Led Zeppelin\", \"The Beatles\", \"Rolling Stones\", \"Radiohead\"],\n",
    "        \"No Members\":[4, 4, 4, 5],\n",
    "        \"No Albums\":[9, 12, 29 ,9]\n",
    "    })\n",
    "bandInfo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe was initialized with a dictonary of column headers as keys and column data as values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a series, a data frame has an index, which corresponds to the first column here. In this case the index was automatically generated, but as for the series, we could use explicit values for the index. \n",
    "\n",
    "We can access columns in a data frame, which returns a series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandInfo[\"Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bandInfo[\"Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And obviously, we can do all the things we've learned about to this column/series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example used columns to create the data frame. We can also create a data frame from rows. This doesn't make a ton of sense in this example, but you could find the data coming out of a data source, like our CSV file, in that order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandInfo2 = pd.DataFrame([\n",
    "        {\"Name\":\"Led Zeppelin\", \"No Albums\":9, \"No Members\":4},\n",
    "        {\"Name\":\"The Beatles\", \"No Albums\":12, \"No Members\":4},\n",
    "        {\"Name\":\"Rolling Stones\", \"No Albums\":29, \"No Members\":4},\n",
    "        {\"Name\":\"Radiohead\", \"No Albums\":9, \"No Members\":5},\n",
    "    ])\n",
    "bandInfo2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a series has only one axis, a dataframe has two, one for the rows (the index or '0' axis), one for the columns (the column or '1' axis). We can check out these axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandInfo.axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access these axes directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The row axis\n",
    "bandInfo.axes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns axis\n",
    "bandInfo.axes[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Data Frames\n",
    "\n",
    "You might have noticed that data frames are rendered in nice HTML tables within Jupyter Notebooks. For small data frames, just showing all the data makes sense, but for larger datasets, like our `hit_albums` dataset, plotting 70+ rows can be annoying, and for datasets with hundreds or thousands of rows it can be prohibitive. By default, a data frame only prints a limited number of elements (notice the `...` in row 30 of the output of `hit_albums` above - only the first 30 and last 30 are printed. \n",
    "\n",
    "When working with data, e.g., when transforming or loading a dataset, it is important to see the raw data, for example, to check if a transformation was done correctly. Often, however, it's sufficient to see a part of the data, e.g., the first couple of rows and/or the last couple of rows. We can do this with the `head()` and `tail()` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head shows the first 5 rows of a datset\n",
    "hit_albums.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can specify how much to show\n",
    "hit_albums.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tail shows the last five rows in a datasaet\n",
    "hit_albums.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check out the dimensions of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we learn that our dataset has 77 rows and 6 columns.\n",
    "\n",
    "We can also get more info about the dataset using the info method, which is especially helpful to see the data types of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for series, we can get a rough description of the numerical values of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't see any descriptions of the columns of non-numerical type. We can, however, get a summary by directly accessing a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums[\"Artist\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that Michael Jackson is the top artist in this list, with five albums. Are there other artists with multiple albums in the list? We can answer that question with the value_counts() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums[\"Artist\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at whether the numerical columns in our data frame are correlated using the [`corr()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html) method. By default, this calculates a Pearson correlation between the column, excluding NaN values. Not surprisingly, we see a rather strong correlation (0.81) between certified and claimed sales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do transpose a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing Data Frames\n",
    "\n",
    "A common task is to create subsets of a dataframe. \n",
    "\n",
    "We can explicitly define the the columns we want by their lables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need to keep the columns we care about\n",
    "hit_albums = hit_albums[[\"Artist\",\"Certified sales (millions)\", \"Claimed sales (millions)\"]]\n",
    "hit_albums.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set a column to be the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums_reindexed = hit_albums.set_index(\"Artist\")\n",
    "hit_albums_reindexed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these access methods we can also update the order: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums[[\"Certified sales (millions)\", \"Artist\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve rows using the `loc` indexer by name. Note that this doesn't work if we use duplicates as indexers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums_reindexed.loc[\"Green Day\":\"Supertramp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be combined with slicing columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums_reindexed.loc[\"Green Day\":\"Supertramp\", [\"Certified sales (millions)\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the same thing using `iloc`, i.e., index based slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums_reindexed.iloc[3:5, [0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Data Frames\n",
    "\n",
    "Very often, we want to aggregate data. Given the hit-albums dataset, for example, we might want to ask how many albums each artist in that list has sold in total. We can do these aggregations using the group-by method. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a column to group by, for example, \"Artist\". We can look at the groups created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = hit_albums.groupby(\"Artist\")\n",
    "grouped.groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the keys map to a set of indices. For example, Michael Jackson's albums are found at indices [0, 10, 16, 65, 66].\n",
    "\n",
    "Once we have created these groups, we can specify what to do with it. A very generic solution is the `agg()` function, which we can pass a function to do things with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we pass the sum function, which calcualtes the sum of a list, to the group\n",
    "grouped.agg(sum).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that we've summed up all columns. However, note that NaN plus some number is still NaN. So let's work with a slice of the dataframe instead. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an aggregation with an in-line function definition where we still create the sum, but also multiply by a million. We use a [lambda expression](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions) to define the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.agg(lambda rows : sum([cell * 1000000 for cell in rows])).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda expressions are just a different way of defining a function in line, without assigning it a name. They only work for a single statement. \n",
    "\n",
    "Here is a simple lambda expression, which returns a function, which we assign to the variable add:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add = lambda a, b : a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also has a couple of built in functions in place of the generic agg method. For example, we can just call `sum()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we create a new slice of the dataset\n",
    "artists_sales = hit_albums[[\"Artist\",\"Certified sales (millions)\"]]\n",
    "# then we group by Artist, then sum the values up\n",
    "aggregated = artists_sales.groupby(\"Artist\").sum()\n",
    "aggregated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we sort them, and have a nice result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated.sort_values(\"Certified sales (millions)\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Data Frames\n",
    "\n",
    "* Calculate the mean certified sales for all albums.\n",
    "* Create a new dataframe that only contains albums with more than 20 million certified sales.\n",
    "* Create a new dataframe based on the hit_albums dataset that only contains the artists that have at least two albums in the list.\n",
    "* Create a new dataframe that contains the aggregates sum of all certified sales for each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buit-in Plotting\n",
    "\n",
    "Dataframes have built-in plotting capabilities based on the [matplotlib](http://matplotlib.org/) library. We'll see more about plotting later - here we'll only use the buit-in capabilities of pandas. \n",
    "\n",
    "First, we have to import the matplotlib library, and tell Jupyter to display the images directly here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "# This next line tells jupyter to render the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can simply call the plot attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default is a line chart. This doesn't make much sense, since it's mixing years with sales. We're better of plotting only the two different sales figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums[[\"Certified sales (millions)\", \"Claimed sales (millions)\"]].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use bar-charts instead of line-charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums[[\"Certified sales (millions)\", \"Claimed sales (millions)\"]].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way to compare certified and claimed sales is a scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.plot.scatter(x=\"Certified sales (millions)\", y=\"Claimed sales (millions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We'll do more sophisticated plotting next time! "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nbpresent": {
   "slides": {
    "19a6495f-8346-4b23-a98a-c7115941e8f0": {
     "id": "19a6495f-8346-4b23-a98a-c7115941e8f0",
     "prev": null,
     "regions": {
      "d6523b36-7204-4001-8a6c-37c431b18d26": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "id": "d6523b36-7204-4001-8a6c-37c431b18d26"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
